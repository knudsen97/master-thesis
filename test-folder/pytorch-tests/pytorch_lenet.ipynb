{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning pytorch with CNN and the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class Cifar10(torchvision.datasets.CIFAR10):\n",
    "  def __init__(self, train: bool, normalize=False):\n",
    "    super().__init__(root='/data', train=train, download=True)\n",
    "    self.data = self.data.astype(np.float32)\n",
    "    if normalize:\n",
    "      self.data = self.data / 255 - 0.5\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.data[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "normalize = False # True\n",
    "data_train = Cifar10(train=True, normalize=normalize)\n",
    "data_train, data_valid = torch.utils.data.random_split(data_train, (45000, 5000))\n",
    "data_test = Cifar10(train=False, normalize=normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'batch_size': 100, 'num_workers': 2}\n",
    "loader_train = torch.utils.data.DataLoader(data_train, **kwargs, shuffle=True)\n",
    "loader_valid = torch.utils.data.DataLoader(data_valid, **kwargs)\n",
    "loader_test = torch.utils.data.DataLoader(data_test, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LeNet CNN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_channels, classes) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Input size & output size\n",
    "        # N = 32*32*3\n",
    "        # n_classes = 10\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.dropout_rate = 0.5\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "        # Define activation function & softmax layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Define max pooling layer\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "\n",
    "\n",
    "        # Define convolutional layers\n",
    "        # nn.Conv2d()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=32, kernel_size=(5,5))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=128, kernel_size=(5,5))\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=128*5*5, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=classes)\n",
    "\n",
    "        # self.fc1 = nn.Linear(in_features=50*5*5, out_features=500)\n",
    "        # self.fc2 = nn.Linear(in_features=500, out_features=classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # permutate input to match pytorch\n",
    "        # From: [batch_size, height, width, channels]\n",
    "        # To:   [batch_size, channels, height, width]\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Pass input through convolutions and max pooling layers\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        # Flatten the output of the convolutional layers\n",
    "        out = torch.flatten(out, 1)\n",
    "        # out = out.view(-1, 50*5*5)\n",
    "\n",
    "        # Pass the flattened output to the fully connected layers\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "def train(model, loader, optimizer, loss_fn):\n",
    "    epoch_losses = []\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    # Begin training\n",
    "    model.train()\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate loss and accuracy\n",
    "        epoch_losses.append(loss.item())\n",
    "        total += len(x)\n",
    "        correct += (torch.argmax(y_pred, dim=1) == y).sum().item()\n",
    "\n",
    "    return epoch_losses, correct / total\n",
    "    \n",
    "def evaluate(model, loader, loss_fn):\n",
    "    epoch_losses = []\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    # Begin validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            # Calculate loss and accuracy\n",
    "            epoch_losses.append(loss.item())\n",
    "            total += len(x)\n",
    "            correct += (torch.argmax(y_pred, dim=1) == y).sum().item()\n",
    "    \n",
    "    return epoch_losses, correct / total\n",
    "\n",
    "\n",
    "\n",
    "def optimize_model(model, optimizer, loss_fn, sched):\n",
    "    train_losses, train_accuracies = [], []\n",
    "    valid_losses, valid_accuracies = [], []\n",
    "\n",
    "    t = tqdm(range(40))\n",
    "    for epoch in t:\n",
    "        losses, acc = train(model, loader_train, optimizer, loss_fn)\n",
    "        train_losses.append(np.mean(losses))\n",
    "        train_accuracies.append(acc)\n",
    "\n",
    "\n",
    "        losses, acc = evaluate(model, loader_valid, loss_fn)\n",
    "        valid_losses.append(np.mean(losses))\n",
    "        valid_accuracies.append(acc)\n",
    "\n",
    "\n",
    "        if sched is not None:\n",
    "            sched.step(valid_losses[-1])\n",
    "\n",
    "        t.set_description(f'loss, val: {valid_losses[-1]:.2f}, acc, val: {valid_accuracies[-1]:.2f}')\n",
    "    \n",
    "    return train_losses, valid_losses, train_accuracies, valid_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(train_losses, valid_losses, train_accuracies, valid_accuracies): \n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    p = plt.plot(train_losses, label='train')\n",
    "    plt.plot(valid_losses, label='valid')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    p = plt.plot(train_accuracies, label='train')\n",
    "    plt.plot(valid_accuracies, label='valid')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49745e29157e4a2c87eeb86e793cb7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model and optimizer\n",
    "model = LeNet(num_channels=3, classes=10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "loss_fn = nn.NLLLoss()\n",
    "sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "\n",
    "tl, vl, ta, va = optimize_model(model, optimizer, loss_fn, sched)\n",
    "visualize_results(tl, vl, ta, va)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac1cf0a5f4d82cc9e64f5357fd50c25b92953cd3a1b6c1fe8e8789bbd41aad43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
